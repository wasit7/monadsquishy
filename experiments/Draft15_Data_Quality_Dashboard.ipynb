{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas numpy pyarrow pandarallel altair"
      ],
      "metadata": {
        "id": "m-qhTcekyx1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Draft15 MONADSQUISHY VISUAL - WITH VEGA-LITE REPORTS\n",
        "# =============================================================================\n",
        "\n",
        "try:\n",
        "    import pandarallel\n",
        "    import altair as alt\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pyarrow as pa\n",
        "except ImportError:\n",
        "    # !pip install pandas numpy pyarrow pandarallel altair\n",
        "    pass\n",
        "\n",
        "# --- 1. SETTINGS ---\n",
        "try:\n",
        "    from pandarallel import pandarallel\n",
        "    pandarallel.initialize(progress_bar=True, verbose=0)\n",
        "    HAS_PARALLEL = True\n",
        "except ImportError:\n",
        "    HAS_PARALLEL = False\n",
        "\n",
        "# Ensure Altair renders in the notebook\n",
        "try:\n",
        "    alt.renderers.enable('default')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "TYPE_SPECS = {\n",
        "    \"string\":   {\"default\": None,   \"dtype\": pa.string(),           \"coerce\": lambda x: x.astype(str)},\n",
        "    \"float\":    {\"default\": np.nan, \"dtype\": pa.float64(),         \"coerce\": lambda x: pd.to_numeric(x, errors='coerce')},\n",
        "    \"datetime\": {\"default\": pd.NaT, \"dtype\": pa.timestamp('ns'), \"coerce\": lambda x: pd.to_datetime(x, errors='coerce')}\n",
        "}\n",
        "\n",
        "# --- 2. CORE ENGINE ---\n",
        "class Monad:\n",
        "    __slots__ = ['value', 'input_row', 'output_column', 'status', 'final_value', 'logs', 'stopped', 'step']\n",
        "    def __init__(self, value, row, col):\n",
        "        self.value = value; self.input_row = row; self.output_column = col\n",
        "        self.status = 'pending'; self.final_value = None; self.logs = []; self.stopped = False; self.step = 0\n",
        "\n",
        "    def _log(self, func, status, details=None):\n",
        "        role = getattr(func, '_role', 'validator')\n",
        "        name = getattr(func, '__name__', 'unknown')\n",
        "        self.logs.append({'row': self.input_row, 'col': self.output_column, 'step': self.step, 'role_type': role, 'role': name, 'status': status, 'details': details})\n",
        "\n",
        "    def __or__(self, func):\n",
        "        if self.stopped:\n",
        "            self.step += 1; self._log(func, 'skipped'); return self\n",
        "        self.step += 1\n",
        "        role = getattr(func, '_role', 'validator')\n",
        "        try:\n",
        "            res = func(self.value)\n",
        "            if role == 'validator':\n",
        "                self.value = res; self.status = 'valid'; self._log(func, 'passed')\n",
        "            elif role == 'transformer':\n",
        "                self.value = res; self.status = 'success'; self.final_value = res; self.stopped = True; self._log(func, 'passed')\n",
        "        except Exception as e:\n",
        "            if role == 'validator':\n",
        "                self.status = 'dirty'; self.final_value = None; self.stopped = True; self._log(func, 'failed', str(e))\n",
        "            elif role == 'transformer':\n",
        "                self._log(func, 'failed', str(e))\n",
        "        return self\n",
        "\n",
        "    def apply(self, pipeline):\n",
        "        transformer_seen = False\n",
        "        for func in pipeline:\n",
        "            if getattr(func, '_role', 'validator') == 'transformer': transformer_seen = True\n",
        "            self | func\n",
        "\n",
        "        if self.status == 'success': pass\n",
        "        elif self.status == 'dirty': self.final_value = None\n",
        "        elif self.status in ['valid', 'pending']:\n",
        "            if transformer_seen:\n",
        "                self.status = 'dirty'; self.final_value = None\n",
        "                # This step is critical: It marks the \"End of Line\" failure\n",
        "                self.logs.append({'row': self.input_row, 'col': self.output_column, 'step': self.step + 1, 'role_type': 'system', 'role': 'chain_exhausted', 'status': 'failed', 'details': 'All transformers failed'})\n",
        "            else:\n",
        "                self.status = 'success'; self.final_value = self.value\n",
        "        return self\n",
        "\n",
        "class SquishyEngine:\n",
        "    def __init__(self, config_list, source_df):\n",
        "        self.config = config_list; self.df = source_df; self.logs = []; self.final_df = None\n",
        "\n",
        "    def run(self):\n",
        "        print(f\"Processing {len(self.df)} rows...\")\n",
        "        final_df = self.df.copy()\n",
        "        all_logs = []\n",
        "        for col_def in self.config:\n",
        "            target = col_def['target']; source = col_def.get('source', target)\n",
        "            pipeline = col_def['pipeline']; spec = TYPE_SPECS.get(col_def.get('type', 'string'))\n",
        "            if source not in self.df.columns: continue\n",
        "\n",
        "            def process(row):\n",
        "                m = Monad(row[source], row.name, target).apply(pipeline)\n",
        "                return (m.final_value if m.status == 'success' else spec['default'], m.logs)\n",
        "\n",
        "            if HAS_PARALLEL: results = self.df.parallel_apply(process, axis=1)\n",
        "            else: results = self.df.apply(process, axis=1)\n",
        "\n",
        "            final_df[target] = results.apply(lambda x: x[0])\n",
        "            for log_list in results.apply(lambda x: x[1]): all_logs.extend(log_list)\n",
        "            final_df[target] = spec['coerce'](final_df[target])\n",
        "\n",
        "        self.final_df = final_df\n",
        "        self.logs = pd.DataFrame(all_logs)\n",
        "        return final_df\n",
        "\n",
        "# --- 3. TOOLKIT ---\n",
        "def validator(f): f._role = 'validator'; return f\n",
        "def transformer(f): f._role = 'transformer'; return f\n",
        "\n",
        "@validator\n",
        "def must_exist(v):\n",
        "    if pd.isna(v) or str(v).strip() == '' or str(v).lower() in ['n/a', 'null', 'nan']: raise Exception(\"Missing\")\n",
        "    return v\n",
        "\n",
        "@transformer\n",
        "def to_iso_code(v):\n",
        "    m = {'TH': 'TH', 'USA': 'US', 'UK': 'UK', 'JP': 'JP', 'CN': 'CN'}\n",
        "    res = m.get(str(v).strip().upper(), None)\n",
        "    if res is None: raise Exception(\"Mapping failed\")\n",
        "    return res\n",
        "\n",
        "@transformer\n",
        "def to_float(v): return float(str(v).replace(',', '').replace('$', '').replace('à¸¿', ''))\n",
        "\n",
        "@transformer\n",
        "def parse_iso(v): return pd.to_datetime(v, format='%Y-%m-%d')\n",
        "@transformer\n",
        "def parse_us(v): return pd.to_datetime(v, format='%m/%d/%Y')\n",
        "@transformer\n",
        "def parse_uk(v): return pd.to_datetime(v, format='%d/%m/%Y')\n",
        "@transformer\n",
        "def parse_thai(v):\n",
        "    s = str(v); parts = s.split('-')\n",
        "    if len(parts) == 3 and int(parts[0]) > 2400:\n",
        "        return pd.to_datetime(f\"{int(parts[0])-543}-{parts[1]}-{parts[2]}\")\n",
        "    raise Exception(\"Not Thai\")\n",
        "\n",
        "# --- 4. DATA GENERATOR ---\n",
        "def generate_complex_data(n=3000):\n",
        "    print(f\"Generating {n} rows of complex data...\")\n",
        "    dates = np.random.choice(['2024-01-01', '01/31/2024', '31/01/2024', '2567-01-01', 'NotDate'], n, p=[0.6, 0.1, 0.1, 0.1, 0.1])\n",
        "    countries = np.random.choice(['TH', 'USA', 'UK', 'JP', 'BadCode', None], n, p=[0.6, 0.2, 0.1, 0.05, 0.025, 0.025])\n",
        "    prices = np.random.choice(['100', '$50.00', '1,000', 'Free', None], n, p=[0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "    return pd.DataFrame({'date_col': dates, 'country_col': countries, 'price_col': prices})\n",
        "\n",
        "# --- 5. VISUAL DASHBOARD (VEGA-LITE) ---\n",
        "def show_visual_dashboard(engine, df_orig, config):\n",
        "    if engine.logs.empty: print(\"No logs generated.\"); return\n",
        "\n",
        "    print(\"\\nGenerating Vega-Lite Reports...\")\n",
        "    charts = []\n",
        "\n",
        "    # --- CHART 1: Quality Summary ---\n",
        "    summary_data = []\n",
        "    for col in engine.logs['col'].unique():\n",
        "        col_logs = engine.logs[engine.logs['col'] == col]\n",
        "        missing = col_logs[(col_logs['role'] == 'must_exist') & (col_logs['status'] == 'failed')]['row'].nunique()\n",
        "        dirty = col_logs[(col_logs['status'] == 'failed') & ((col_logs['role_type'] == 'validator') | (col_logs['role'] == 'chain_exhausted'))]\n",
        "        dirty = dirty[dirty['role'] != 'must_exist']['row'].nunique()\n",
        "        passed = len(df_orig) - missing - dirty\n",
        "\n",
        "        summary_data.append({'Column': col, 'Status': 'Passed', 'Count': passed})\n",
        "        summary_data.append({'Column': col, 'Status': 'Missing', 'Count': missing})\n",
        "        summary_data.append({'Column': col, 'Status': 'Invalid', 'Count': dirty})\n",
        "\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "    # Explicit sorting for the stacked bar: Passed (Left) -> Invalid (Middle) -> Missing (Right)\n",
        "    # We use a numeric rank to force this order in Altair\n",
        "    status_order = {'Passed': 0, 'Invalid': 1, 'Missing': 2}\n",
        "    df_summary['rank'] = df_summary['Status'].map(status_order)\n",
        "\n",
        "    chart_quality = alt.Chart(df_summary, title=\"1. Data Quality Summary\").mark_bar().encode(\n",
        "        x=alt.X('Count:Q', stack='normalize', axis=alt.Axis(format='%')),\n",
        "        y=alt.Y('Column:N'),\n",
        "        color=alt.Color('Status:N', scale=alt.Scale(domain=['Passed', 'Missing', 'Invalid'], range=['#2ca02c', '#ff7f0e', '#d62728'])),\n",
        "        order=alt.Order('rank', sort='ascending'),\n",
        "        tooltip=['Column', 'Status', 'Count']\n",
        "    ).properties(width=600, height=150)\n",
        "    charts.append(chart_quality)\n",
        "\n",
        "    # --- CHART 2: Operation Chain Analysis ---\n",
        "    chain_data = []\n",
        "    for col in engine.logs['col'].unique():\n",
        "        # Group by step, role AND role_type to differentiate validators\n",
        "        grp = engine.logs[engine.logs['col'] == col].groupby(['step', 'role', 'role_type'])['status'].value_counts().reset_index(name='Count')\n",
        "        grp['Column'] = col\n",
        "\n",
        "        # Logic to separate Validator Pass (Blue) vs Transformer Pass (Green)\n",
        "        def get_display_status(row):\n",
        "            if row['status'] == 'passed' and row['role_type'] == 'validator':\n",
        "                return 'validation_pass'\n",
        "            return row['status']\n",
        "\n",
        "        grp['display_status'] = grp.apply(get_display_status, axis=1)\n",
        "\n",
        "        # Explicit Ranking for Y-Axis Sorting (Pipeline Step)\n",
        "        grp['rank'] = grp['step']\n",
        "        grp.loc[grp['role'] == 'chain_exhausted', 'rank'] = 999\n",
        "\n",
        "        # Explicit Ranking for Stack Order\n",
        "        # 0. Skipped (Left/Bottom)\n",
        "        # 1. Validation Pass (Blue)\n",
        "        # 2. Transformer Pass (Green)\n",
        "        # 3. Failed (Right/Top)\n",
        "        status_map = {'skipped': 0, 'validation_pass': 1, 'passed': 2, 'failed': 3}\n",
        "        grp['status_rank'] = grp['display_status'].map(status_map)\n",
        "\n",
        "        chain_data.append(grp)\n",
        "\n",
        "    df_chain = pd.concat(chain_data)\n",
        "\n",
        "    chart_chain = alt.Chart(df_chain, title=\"2. Operation Chain Flow (Pass/Fail/Skip)\").mark_bar().encode(\n",
        "        x=alt.X('Count:Q', stack='normalize'),\n",
        "        y=alt.Y('role:N', sort=alt.EncodingSortField(field=\"rank\", op=\"min\", order=\"ascending\"), title=\"Pipeline Step\"),\n",
        "        # Update Color Scale to include Blue for Validation Pass\n",
        "        color=alt.Color('display_status:N',\n",
        "                        scale=alt.Scale(\n",
        "                            domain=['passed', 'validation_pass', 'failed', 'skipped'],\n",
        "                            range=['#2ca02c', '#1f77b4', '#d62728', '#c7c7c7']\n",
        "                        ),\n",
        "                        legend=alt.Legend(title=\"Status\")\n",
        "        ),\n",
        "        # Apply the custom sort order for the stack\n",
        "        order=alt.Order('status_rank', sort='ascending'),\n",
        "        row=alt.Row('Column:N', header=alt.Header(titleOrient=\"top\", labelOrient=\"top\")),\n",
        "        tooltip=['Column', 'step', 'role', 'display_status', 'Count']\n",
        "    ).properties(width=600, height=100).resolve_scale(y='independent')\n",
        "    charts.append(chart_chain)\n",
        "\n",
        "    # --- CHART 3: Recommender ---\n",
        "    stats = engine.logs.groupby(['col', 'role', 'role_type'])['status'].value_counts().unstack(fill_value=0)\n",
        "    if 'passed' not in stats.columns: stats['passed'] = 0\n",
        "    stats['E-Score'] = stats['passed'] / len(df_orig)\n",
        "    trans_stats = stats[stats.index.get_level_values('role_type') == 'transformer'].reset_index()\n",
        "\n",
        "    # Create a base chart to share encoding between bars and text\n",
        "    base = alt.Chart(trans_stats).encode(\n",
        "        x=alt.X('E-Score:Q', scale=alt.Scale(domain=[0, 1])),\n",
        "        y=alt.Y('role:N', sort='-x'),\n",
        "        tooltip=['col', 'role', 'E-Score']\n",
        "    )\n",
        "\n",
        "    # 1. Green bars\n",
        "    bars = base.mark_bar().encode(\n",
        "        color=alt.value('#2ca02c')\n",
        "    )\n",
        "\n",
        "    # 2. Text labels showing the output column (Context)\n",
        "    text = base.mark_text(\n",
        "        align='left',\n",
        "        baseline='middle',\n",
        "        dx=3,  # Shift text slightly to the right of the bar\n",
        "        color='black'\n",
        "    ).encode(\n",
        "        text='col:N'\n",
        "    )\n",
        "\n",
        "    # Combine bars and text\n",
        "    chart_rec = (bars + text).properties(\n",
        "        title=\"3. AI Recommender (Efficiency Scores)\",\n",
        "        width=600,\n",
        "        height=200\n",
        "    )\n",
        "\n",
        "    charts.append(chart_rec)\n",
        "\n",
        "    # --- DISPLAY ---\n",
        "    final_dashboard = alt.vconcat(*charts).resolve_scale(color='independent')\n",
        "    display(final_dashboard)\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def main():\n",
        "    df = generate_complex_data(3000)\n",
        "    config = [\n",
        "        # OPTIMIZED ORDER: ISO (Most common) is FIRST\n",
        "        {\"target\": \"clean_date\", \"source\": \"date_col\", \"type\": \"datetime\", \"pipeline\": [must_exist, parse_iso, parse_us, parse_uk, parse_thai]},\n",
        "        {\"target\": \"country_code\", \"source\": \"country_col\", \"type\": \"string\", \"pipeline\": [must_exist, to_iso_code]},\n",
        "        {\"target\": \"price\", \"source\": \"price_col\", \"type\": \"float\", \"pipeline\": [must_exist, to_float]}\n",
        "    ]\n",
        "    engine = SquishyEngine(config, df)\n",
        "    engine.run()\n",
        "    show_visual_dashboard(engine, df, config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GqGYSH1byxSK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3_4fv9ZzHZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}